---
title: "Examples of Bayesian Analyses"
author: "Dave Bridges"
date: "2024-08-12"
format: 
  html:
    toc: true
    toc-location: right
theme: journal
execute:
  keep-md: true
  message: false
  warning: false
---

## Some Common Analyses

As a general rule we do four analyses fairly commonly in the Bridges Lab, summarized here based on the nature of the independent and dependent variables:

| Dependent Variable        | Independent Variable      | Analysis                                    |
|------------------------|--------------------------|-----------------------|
| Continuous                | Continuous                | Linear Regression                           |
| Continuous                | Counts (Yes/No or Groups) | Binomial Regression                         |
| Counts (Yes/No or Groups) | Continuous                | Pairwise Test (*t*-test/Mann-Whitney/ANOVA) |
| Counts (Yes/No or Groups) | Counts (Yes/No or Groups) | $\chi^2$ or Fisher's Test                   |

Lets take a Bayesian approach to each of these using the Mtcars dataset, which looks like this after a bit of fiddling.

```{r mtcars}
library(knitr)
library(dplyr)
mtcars.data <- 
  as.data.frame(mtcars) %>%
  mutate(transmission = factor(case_when(am==0~"automatic",
                           am==1~"manual")),
         engine=factor(case_when(vs==0~"V-Shaped",
                       vs==1~"Straight")),
         cylindars = factor(cyl))
  
kable(mtcars.data,caption="The mtcars dataset")
```

## What is a Pairwise Testing Equivalent?

Lets start by testing if there is a relationship between one quantitative variables, the mpg (miles per gallon) and a categorical variable - transmission (manual or automatic transmission) using the default priors

```{r brms-pairwise}
#| output: false
library(brms)
pairwise.fit <- brm(mpg~transmission,data=mtcars.data,
                    sample_prior = TRUE)
```

```{r pairwise-analysis}
library(broom.mixed)
tidy(pairwise.fit) %>% kable(caption="Summary of model fit for mpg versus transmission")

plot(pairwise.fit)
hypothesis(pairwise.fit, "transmissionmanual>0") # testing for whether manual tramsmission has higher mpg
```

As you can see, this analysis estimates that the manual transmission has a `r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Estimate` higher mpg ($\pm$ `r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Est.Error`) with a very high Bayes Factor (`r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Evid.Ratio`) and posterior probability (`r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Post.Prob`).

The posterior distribution for the effect of a manual transmission is here:

```{r pairwise-posterior}
library(ggplot2)
as_draws_df(pairwise.fit) %>%
  ggplot(aes(x=b_transmissionmanual)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Effect of manual transmission (mpg)",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

We didnt specify the model type (gausian is the default). We also used the default priors here, which were

```{r pairwise-priors}
prior_summary(pairwise.fit) %>% kable(caption="Default priors for a pairwise analysis of mpg vs transmission in the mtcars data")
```

This includes flat priors for the beta coefficients, student's *t* distributions for intercept (centered around the mean mpg) and residual error (centered around zero).

### How about an ANOVA

Lets say we want to test across groups (rather than testing effects between groups)
.  For that we can use the number of cylindars as a factor


```{r brms-anova}
#| output: false
anova.fit <- brm(mpg~0+cylindars,data=mtcars.data, #zero makes sure each cylindar gets its own intercept
                    sample_prior = TRUE)
anova.fit.null <- brm(mpg~0,data=mtcars.data, #null model not including cylindars
                    sample_prior = TRUE)
```

The analysis here is a little bit different than before.  We can still plot the posterior distributions for each model but the relevant test is now to compare the model with the cylindars to a null model, without that term.

```{r brms-anova-analysis}
tidy(anova.fit) %>% kable(caption="Summary of model fit for mpg versus cylindars")

plot(anova.fit)
# extracts the bayes factor comparing the models
bayes_factor(anova.fit,anova.fit.null) -> anova.bf 

library(tidyr)
as_draws_df(anova.fit) %>%
  select(starts_with('b')) %>%
  pivot_longer(cols=everything(),
               names_to = "term",
               values_to="mpg") %>%
  ggplot(aes(x=mpg,fill=term)) +
  geom_density() +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Miles Per Gallon",
       title="Posterior probabilities") +
  scale_fill_discrete(name="") +
  theme_classic(base_size=16) +
  theme(legend.position="top")
```

In this case the Bayes Factor for the hypothesis that this term is relevant is `r anova.bf$bf`.

We can still do post-hoc tests using the hypothesis command:

```{r brms-anova-posthoc}
rbind(hypothesis(anova.fit, "cylindars4>cylindars6")$hypothesis,
      hypothesis(anova.fit, "cylindars4>cylindars8")$hypothesis,
      hypothesis(anova.fit, "cylindars6>cylindars8")$hypothesis) %>%
  kable(caption="Pairwise hypothesis tests for cylindars on mpg")
```


## What is a Linear Regression Equivalent?

Lets start by testing if there is a relationship between two quantitative variables, the mpg (miles per gallon) and the weight (wt) using the default priors

```{r brms-linear-regression}
#| output: false
linear.fit <- brm(mpg~wt,data=mtcars,
                  sample_prior = TRUE)
```

Lets see what this looks like

```{r linear-analysis}
prior_summary(linear.fit) %>% kable(caption="Prior summary for effects of weight on mpg")
tidy(linear.fit) %>% kable(caption="Summary of model fit for mpg versus weight")

plot(linear.fit)
hypothesis(linear.fit, "wt<0") # testing for whether weight has a negative effect on mpg

as_draws_df(linear.fit) %>%
  ggplot(aes(x=b_wt)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Effect of Weight (mpg/1000 kg)",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

Notice that the priors were the same here (they are based on the dependent variable), and again there is a highly probable inverse relationship between mpg and weight (heavier cars get lower fuel economy).

### What if I am more interested in the R-squared?

To do this we can use the `bayes_R2` function.

```{r r2-calculations}
kable(bayes_R2(linear.fit),caption="Estimates for R2 between weight and mpg")
r2.probs <- bayes_R2(linear.fit, summary=F) #summary false is to get the posterior probabilities
ggplot(data=r2.probs, aes(x=R2)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="R2",
       title="Posterior probabilities") +
  lims(x=c(0,1)) +
  theme_classic(base_size=16)
```

## What is a Chi-Squared Equivalent

First lets do an example with a standard $\chi^2$ test (and a Fisher's test since the counts are quite low) on whether there is a relationship bewteen engine type and transmission type.

```{r brms-chisq-data}
library(tidyr) #for pivot wider
library(tibble) #for column to rowname
engine.trans.counts <-
  mtcars.data %>%
  group_by(engine,transmission) %>%
  count() %>%
  pivot_wider(names_from=transmission,values_from=n) %>%
  column_to_rownames('engine')

chisq.test(engine.trans.counts) %>% tidy %>% kable(caption="Chi-squared test for engine/transmission")
fisher.test(engine.trans.counts) %>% tidy %>% kable(caption="Fisher's test for engine/transmission")
```

Both agree, not much of a relationship here. For the brms modelling we need to make a few changes. First, we will use a bernoulli distribution since our data is only zeros and ones, again we will use all the default priors.

```{r brms-chisq}
#| output: false
# Fit the model
counts.model <- brm(engine ~ transmission, 
           data = mtcars.data, 
           family = bernoulli())
```

Lets look at these results

```{r chisq-analysis}
prior_summary(counts.model) %>% kable(caption="Prior summary for effects of transmission on engine type")
tidy(counts.model) %>% kable(caption="Summary of model fit for transmission versus engine type")

plot(counts.model)
hypothesis(counts.model, "transmissionmanual<0") # testing for whether weight has a negative effect on mpg

as_draws_df(counts.model) %>%
  ggplot(aes(x=b_transmissionmanual)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Beta coefficient for manual transmission",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

Now in this case there is moderate evidence for a negative relationship between transmnission and engine type ($\beta$=`r fixef(counts.model)['transmissionmanual',"Estimate"]` $\pm$ `r fixef(counts.model)['transmissionmanual',"Est.Error"]`) with a Bayes Factor of `r hypothesis(counts.model, "transmissionmanual<0")$hypothesis$Evid.Ratio` and a Posterior Probability of `r hypothesis(counts.model, "transmissionmanual<0")$hypothesis$Post.Prob`.

## What is a Binomial Regression Equivalent?

Lets now look at the relationsbip between transmission type (binomial variable) and weight (continuous variable). Again we will use a bernouli distribution (which will use, by default a logit link function)

```{r brms-binomial}
#| output: false
# Fit the model
binomial.fit <- brm(transmission ~ wt, 
           data = mtcars.data, 
           family = bernoulli())
```

```{r binomial-analysis}
prior_summary(binomial.fit) %>% kable(caption="Prior summary for effects of transmission on engine type")
tidy(binomial.fit) %>% kable(caption="Summary of model fit for transmission versus engine type")

plot(binomial.fit)
hypothesis(binomial.fit, "wt<0") # testing for whether weight has a negative effect on mpg

as_draws_df(binomial.fit) %>%
  ggplot(aes(x=b_wt)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Beta coefficient for weight on transmission",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

Again there is strong evidence that a higher weight makes an automatic transmission more likely.

Hopefully these examples helped, but a couple things since we used defaults.

Think closely about your priors, if you have a good reason to set them to something other than the default you should. In this case the defaults worked pretty well and as you can see are set based on the data that is input. These are generally very **weakly informative priors** and the modelling could be improved on by setting your own.

Also remember, nowhere in here was there a p-value. We could consider a posterior probability of \>0.95 our cutoff for significance if we prefer, but make sure to state this in your methods (along with your choice of priors and the package used.)

Note this script used some examples generated by [perplexity.ai](https://www.perplexity.ai/) and then modified further

# Session Info

```{r sessionInfo}
sessionInfo()
```
